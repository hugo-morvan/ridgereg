---
title: "Ridge Regression Vignette"
author: "Simge Cinar & Hugo Morvan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Ridge Regression Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r library}
library(ridgereg)
```

```{r example}
library(ridgereg)
ridgereg_model <- ridgereg:::ridgereg$new(Petal.Length~Species, iris, 0.001)
ridgereg_model$beta_ridge
```

#QUESTION 1.2

### 1): Create a 70-30 train-test split
```{r}
data(Boston, package = "MASS")
set.seed(123)
splitIndex <- caret::createDataPartition(Boston$medv, p = 0.7, list = FALSE)
training_data <- Boston[splitIndex, ]
test_data <- Boston[-splitIndex, ]
# Convert columns to numeric
training_data <- as.data.frame(lapply(training_data, as.numeric))
test_data <- as.data.frame(lapply(test_data, as.numeric))
```

### 2,3) Fit a linear regression model and a fit a linear regression model with forward selection of covariates on the training dataset.
```{r}
# KEEP
# Model1: Full model with the linear regression
mod1 <- lm(medv ~ ., data=training_data)
cat("Mean Square Error (MSE):",((1/nrow(training_data))*sum((mod1$residuals)^2)))
```

```{r}
#KEEP
# Model2: Model with forward selection
# Selects the model with the lowest AIC
fullModel <- lm(medv ~ ., data=training_data)
nullModel <- lm(medv ~ 1, data=training_data)
forward_model <- stepAIC(nullModel, # start with a model containing no variables
                direction = 'forward', # run forward selection
                scope = list(upper = fullModel, 
                             lower = nullModel), 
                trace = 0) # do not show the step-by-step process of model selection


cat("Mean Square Error (MSE):",((1/nrow(training_data))*sum((forward_model$residuals)^2)))
```
